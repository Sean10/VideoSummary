---
title: "Ceph Tech Talk: Karan Singh - Scale Testing Ceph with 10Billion+ Objects 2020-10-01"
date: 2020-10-02
updated: 2020-10-03
tags:
categories:
- "视频总结"
subtitle: tech
---


### 会议纪要

#### 会议概述
本次会议是关于Red Hat的Ceph对象存储在规模测试中的表现，特别是在处理超过10亿个对象时的性能评估。会议由Karan Singh主持，他是Red Hat云存储和数据服务业务单元的高级解决方案架构师。

#### 讨论的主要议题
1. **规模测试的目的和背景**：
   - 本次测试旨在验证Ceph对象存储在处理大规模数据（10亿个对象）时的性能。
   - 之前的测试已经验证了Ceph在处理1亿个对象时的性能，本次测试是对其能力的进一步验证。

2. **测试环境和配置**：
   - 使用了6个Red Hat存储节点，每个节点配备53个16TB的旋转设备和6个Intel QLC 3.9设备。
   - 软件配置包括Red Hat Ceph Storage 4.1，所有守护进程（OSD、Monitor、Manager、RADOS Gateway）都容器化。

3. **测试结果和性能分析**：
   - 在处理小对象（64KB）和大对象（128MB）时，Ceph展示了确定性的性能。
   - 在达到10亿个对象时，系统仍然保持稳定的写入和读取性能。
   - 在硬件故障模拟测试中，系统表现出了良好的容错能力，即使在节点或设备故障的情况下，性能下降也在可接受范围内。

4. **性能优化的建议**：
   - 建议使用多个RADOS Gateway实例以提高性能。
   - 推荐使用4%的闪存容量作为BlueStore的元数据存储。
   - 建议增加OSD内存目标大小以提高性能。

#### 决定的事项
- 确认了Ceph对象存储在处理大规模数据时的稳定性和性能。
- 确定了在设计和部署大规模Ceph集群时的最佳实践和配置建议。

#### 后续行动计划
- 发布完整的测试报告，供社区和客户参考。
- 继续监控和优化Ceph在大规模数据处理中的性能，以支持未来的数据湖和大数据工作负载。

#### 其他信息
- 会议中还讨论了关于BlueStore的配置和优化，以及如何更有效地使用闪存存储。
- 提供了关于如何根据测试结果进行集群规模估算的指导。

本次会议为Ceph对象存储在大规模数据处理中的应用提供了宝贵的性能数据和优化建议，有助于推动Ceph在企业级存储解决方案中的进一步应用。