---
title: "Ceph Code Walkthrough 2020-08-25: kRBD I/O Flow"
date: 2020-08-25
updated: 2020-08-26
tags:
categories:
- "视频总结"
subtitle: tech
---


### 会议纪要：2020年8月Ceph代码走读会议

#### 会议概要
- **时间**：2020年8月
- **主讲人**：Ilya，Ceph维护和内核客户端开发人员
- **主题**：内核RBD驱动程序的映射和解映射以及基本IO流程

#### 讨论内容
1. **内核RBD驱动程序概述**
   - RBD驱动程序位于Linux内核源树的`driver/block`子目录下，包含一个源文件和一个小头文件。
   - 支持大多数Ceph特性，但不支持基于日志的镜像和所谓的实时迁移。
   - 新引入的基于快照的镜像在Octopus版本中被隐式支持。

2. **依赖模块**
   - RBD驱动程序依赖于另一个内核模块`libceph`，这是一个简化版的`librados`库，位于`net/ceph`子目录下。
   - `libceph`实现了认证框架和消息传递协议（如Messenger v1）。

3. **映射和解映射过程**
   - 通过`sysfs`接口进行映射和解映射操作。
   - 映射时，RBD命令行工具构建配置字符串并写入特定属性文件。
   - 解映射时，通过写入设备ID和可选的强制标志来解除映射。

4. **IO处理流程**
   - 入口点是`rbd_rq`函数，该函数由块层调用以处理每个IO请求。
   - IO请求被转换为RBD代码，并初始化为图像请求。
   - 图像请求可能涉及多个对象请求，因为RBD图像被条带化到多个对象上。

5. **对象映射和父图像处理**
   - 读取操作会查询对象映射，如果对象不存在，则处理为空洞。
   - 如果存在父图像，则将对象范围反向映射到父图像并进行读取。

6. **写入状态机**
   - 写入操作涉及对象映射更新和从父图像复制数据（如果需要）。
   - 写入状态机处理对象删除和复制操作，确保数据一致性。

#### 决定事项
- 目前没有计划实现基于日志的镜像和实时迁移功能。
- RBD驱动程序将继续优化其IO处理流程和状态机设计。

#### 后续行动计划
- 继续改进RBD驱动程序的性能和稳定性。
- 探索和解决与容器使用案例相关的问题。

#### 其他讨论
- 关于多队列块层和硬件队列数量的讨论。
- 用户空间与内核空间实现的比较，以及NBD驱动程序的使用场景。

#### 结束语
- Ilya感谢大家的参与，并鼓励大家在Ceph邮件列表或通过电子邮件与他联系以获取更多信息。

---

本次会议详细介绍了Ceph内核RBD驱动程序的工作原理和IO处理流程，为参与者提供了深入的技术洞察，并为进一步的开发和优化工作奠定了基础。