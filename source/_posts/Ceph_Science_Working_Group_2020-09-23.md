---
title: "Ceph Science Working Group 2020-09-23"
date: 2020-09-24
updated: 2020-09-25
tags:
categories:
- "视频总结"
subtitle: tech
---


### 会议纪要

#### 会议概述
本次会议是九月的一个小型聚会，由一群使用Ceph的研究系统管理员或大型集群辅助管理员组成，旨在讨论与Ceph相关的任何话题。会议鼓励参与者自由发言和提出讨论话题。

#### 主要议题与讨论内容
1. **近期故障报告**
   - 一位参与者报告了最近的一次长达四小时的全面停电事件，由于UPS仅能维持20分钟，整个数据中心宕机。其中两个集群在恢复后正常启动，但第三个近10PB的大型集群遇到了问题，需要手动干预才能启动。

2. **Ceph Bug讨论**
   - 讨论了Ceph版本14.2.11和Octopus中的一些问题，特别是OSD map trimming逻辑的错误，该错误导致过早的trimming。此外，还提到了文件存储OSD在PG迁移后未能自动清理的问题，导致磁盘使用率异常高。

3. **Octopus版本迁移经验**
   - 有参与者分享了从Luminous升级到Nautilus的经验，特别是关于S3网关的升级。讨论了升级过程中可能遇到的问题，特别是关于区域设置和客户端兼容性问题。

4. **OpenStack与S3集成**
   - 讨论了如何在OpenStack环境中集成S3，特别是如何处理用户通过OpenStack管理S3凭证和配额的需求。提到了通过同步Keystone中的EC2凭证到Ceph网关来优化性能的方法。

5. **大容量文件管理问题**
   - 讨论了在处理包含大量小文件的S3 bucket时可能遇到的管理和性能问题，特别是在进行resharding操作后，列表操作性能下降的问题。

6. **Ceph-CSI使用情况**
   - 讨论了Ceph CSI在Kubernetes环境中的使用情况，特别是与RBD和CephFS的集成。提到了配置选项和可能的兼容性问题。

#### 决定事项
- 对于Ceph版本14.2.11和Octopus中的已知问题，建议社区成员关注并参与修复。
- 对于大容量文件管理问题，建议进一步研究和测试可能的优化措施。

#### 后续行动计划
- 继续监控和报告Ceph版本中的bug和性能问题。
- 对于S3和OpenStack集成问题，建议进一步研究和分享最佳实践。
- 对于Ceph-CSI的使用，建议社区成员分享更多实际使用经验和配置建议。

#### 会议结束
会议在讨论了所有预定议题后结束，下一次会议计划在两个月后的11月25日举行。会议组织者将提前一周发送会议通知。