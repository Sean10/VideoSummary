---
title: "Designing and Tuning for All-Flash Ceph RBD Storage | Ceph Days NYC 2024"
date: 2024-05-15
updated: 2024-05-15
tags:
categories:
- "视频总结"
subtitle: tech
---


### 会议纪要

#### 会议主题：设计与调优全闪存Ceph RBD存储

#### 主讲人：Tyler

#### 会议概要：
- **主讲人背景**：Tyler来自Bloomberg的计算基础设施团队，负责领导计算基础设施团队，专注于Ceph RBD存储的设计与调优。
- **公司背景**：Bloomberg自2013年开始使用OpenStack和Ceph，设计目标是高密度计算和深度存储。
- **网络架构**：重新设计网络架构，采用基于L3的网络，以解决大规模VM部署中的网络挑战。
- **性能优化**：
  - **内存管理**：发现并解决了内存交换问题，通过调整内存目标和深入分析，最终发现是NUMA架构下的内存分配问题。
  - **网络与存储**：利用L3网络架构和全闪存存储，实现了无客户影响的升级和维护。
  - **NUMA优化**：通过优化NUMA设置，显著降低了上下文切换，提高了性能稳定性。
- **性能挑战与解决方案**：
  - **交换问题**：最初通过降低内存目标暂时解决，但最终发现是NUMA配置问题，通过优化NUMA设置解决了问题。
  - **性能波动**：通过禁用scrubs和优化网络带宽使用，减少了性能波动。
  - **内核与驱动优化**：调整NIC驱动设置，优化了RSS处理，减少了CPU核心的热运行。
  - **CRC32性能**：发现并优化了CRC32计算，通过使用更高效的CRC函数，提高了性能。

#### 决定事项：
- **NUMA优化**：继续深入研究和优化NUMA设置，以进一步提高性能和稳定性。
- **CRC32优化**：考虑在Ceph中默认使用更高效的CRC32实现。
- **网络与存储协同**：继续利用L3网络架构和全闪存存储的优势，优化维护和升级流程。

#### 后续行动计划：
- **性能监控**：加强性能监控，确保及时发现并解决潜在的性能问题。
- **社区贡献**：将优化经验和发现分享给Ceph社区，特别是关于NUMA和CRC32的优化。
- **持续优化**：持续关注和优化Ceph RBD存储的性能，确保满足高密度计算和深度存储的需求。

#### 关键词：
- OpenStack
- Ceph (RBD, OSD, RGW)
- L3 Networking
- NUMA
- CRC32
- All-Flash Storage
- Performance Tuning

#### 结论：
通过深入的性能分析和优化，Bloomberg成功解决了全闪存Ceph RBD存储的性能问题，特别是在内存管理和网络优化方面取得了显著成果。这些经验将为未来的存储优化和社区贡献提供宝贵的参考。